import streamlit as st
import requests
import json
import time
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# --- Configuration des Personnalit√©s (avec mod√®les HF par d√©faut) ---
PERSONNALITES = {
    "Analyste Pragmatique": {
        "prompt": "Tu es un analyste expert. R√©ponds de mani√®re factuelle, en te basant sur des donn√©es et des preuves logiques. D√©compose les probl√®mes de mani√®re structur√©e. Ne laisse pas l'√©motion influencer tes arguments. Sois pr√©cis et objectif.",
        "avatar": "üìä",
        "default_model_ollama": "llama3",
        "default_model_hf": "meta-llama/Llama-3-8B-Instruct"
    },
    "Visionnaire Cr√©atif": {
        "prompt": "Tu es un penseur visionnaire et cr√©atif. Propose des id√©es audacieuses et regarde au-del√† des contraintes actuelles. Utilise des analogies et des m√©taphores pour illustrer tes points. Inspire les autres √† penser diff√©remment.",
        "avatar": "üé®",
        "default_model_ollama": "mistral",
        "default_model_hf": "mistralai/Mistral-7B-Instruct-v0.2"
    },
    "Sceptique Critique": {
        "prompt": "Tu es un esprit critique et sceptique. Ton r√¥le est de questionner les affirmations, d'identifier les risques et les faiblesses dans les arguments des autres. Sois direct, concis et ne prends rien pour acquis.",
        "avatar": "üßê",
        "default_model_ollama": "phi3",
        "default_model_hf": "microsoft/Phi-3-mini-4k-instruct"
    },
    "√âthicien Humaniste": {
        "prompt": "Tu es un √©thicien humaniste. Analyse chaque argument sous l'angle de son impact sur la soci√©t√©, l'individu et les principes moraux. Fais preuve d'empathie et rappelle constamment l'importance des valeurs humaines.",
        "avatar": "‚ù§Ô∏è",
        "default_model_ollama": "gemma",
        "default_model_hf": "google/gemma-1.1-7b-it"
    }
}

# --- Configuration des Th√©matiques ---
THEMATIQUES = [
    "L'IA va-t-elle cr√©er plus d'emplois qu'elle n'en d√©truira ?",
    "L'humanit√© doit-elle investir dans la colonisation de Mars avant de r√©soudre les probl√®mes sur Terre ?",
    "Les r√©seaux sociaux sont-ils une menace ou un outil pour la d√©mocratie ?",
]

# --- Fonctions de Backend ---

# --- Ollama Backend ---
@st.cache_data(ttl=60)
def get_ollama_models():
    try:
        response = requests.get("http://localhost:11434/api/tags")
        response.raise_for_status()
        return [model["name"] for model in response.json().get("models", [])]
    except (requests.exceptions.RequestException, json.JSONDecodeError):
        return []

def call_ollama_model(model_name, system_prompt, history_str, agent_name):
    prompt = f"System: {system_prompt}\n\n{history_str}\n\n**C'est √† ton tour, {agent_name}.**"
    try:
        response = requests.post("http://localhost:11434/api/generate", json={"model": model_name, "prompt": prompt, "stream": False}, timeout=60)
        response.raise_for_status()
        return response.json()["response"]
    except requests.exceptions.RequestException as e:
        st.error(f"Erreur Ollama: {e}")
        return None

# --- Hugging Face Backend ---
@st.cache_resource(max_entries=4) # Cache pour les mod√®les et tokenizers
def get_hf_pipeline(model_id):
    st.write(f"Chargement du mod√®le : {model_id}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        # Utilisation de torch_dtype pour optimiser la m√©moire (surtout pour les gros mod√®les)
        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")
        return pipeline("text-generation", model=model, tokenizer=tokenizer)
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le {model_id}: {e}")
        return None

def call_hf_model(pipe, system_prompt, history, agent_name):
    messages = [
        {"role": "system", "content": system_prompt},
        *history, # D√©paquette l'historique des messages
        {"role": "user", "content": f"C'est √† ton tour, {agent_name}. Continue le d√©bat."}
    ]
    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.95)
    return outputs[0]["generated_text"].split("<|assistant|>")[-1].strip()

# --- Interface Streamlit ---
st.set_page_config(layout="wide")
st.title("DebateLLM ü§ñüí¨")
st.markdown("Simulez un d√©bat entre agents IA avec des mod√®les locaux (Ollama) ou distants (Hugging Face).")

# --- Panneau de Configuration ---
with st.sidebar:
    st.header("Configuration du D√©bat")

    backend_choice = st.radio("1. Choisissez le backend du mod√®le :", ["Ollama (Local)", "Hugging Face (En ligne)"], horizontal=True)

    theme = st.selectbox("2. Choisissez une th√©matique :", THEMATIQUES)
    num_agents = st.slider("3. Nombre d'agents :", 2, 4, 2)
    num_tours = st.slider("4. Nombre de tours de parole par agent :", 1, 5, 2)

    st.markdown("---")
    st.header("Configuration des Agents")
    agents_config = []
    personnalites_disponibles = list(PERSONNALITES.keys())

    if backend_choice == "Ollama (Local)":
        local_models = get_ollama_models()
        if not local_models:
            st.warning("Aucun mod√®le Ollama d√©tect√©. Assurez-vous qu'Ollama est lanc√©.")
        else:
            st.success(f"Mod√®les locaux : {len(local_models)}")

    for i in range(num_agents):
        st.markdown(f"**Agent {i+1}**")
        perso = st.selectbox(f"Personnalit√©", personnalites_disponibles, key=f"perso_{i}")
        
        if backend_choice == "Ollama (Local)":
            if local_models:
                model = st.selectbox("Mod√®le", local_models, key=f"model_{i}")
            else:
                model = st.text_input("Mod√®le", PERSONNALITES[perso]["default_model_ollama"], key=f"model_{i}")
        else: # Hugging Face
            model = st.text_input("Repo ID Hugging Face", PERSONNALITES[perso]["default_model_hf"], key=f"model_{i}")
        
        agents_config.append({"nom": perso, "modele": model})

    start_button = st.button("üöÄ Lancer le D√©bat", use_container_width=True)

# --- Zone de D√©bat ---
if start_button:
    st.header(f"D√©bat sur : *{theme}*")
    conversation_log = []
    
    # Initialisation de l'historique pour les deux backends
    hf_history = []
    ollama_history_str = f"Th√®me du d√©bat : {theme}"

    with st.spinner("Le d√©bat est en cours... Chargement des mod√®les si n√©cessaire."):
        # Pr√©-chargement des pipelines HF pour √©viter de le faire dans la boucle
        if backend_choice == "Hugging Face (En ligne)":
            pipelines = {agent["modele"]: get_hf_pipeline(agent["modele"]) for agent in agents_config}
            if any(p is None for p in pipelines.values()):
                st.error("Un ou plusieurs mod√®les Hugging Face n'ont pas pu √™tre charg√©s. Le d√©bat est annul√©.")
                st.stop()

        for tour in range(num_tours):
            st.subheader(f"Tour de parole n¬∞{tour + 1}")
            for agent in agents_config:
                agent_name = agent["nom"]
                model_name = agent["modele"]
                personality = PERSONNALITES[agent_name]

                with st.chat_message(name=agent_name, avatar=personality["avatar"]):
                    placeholder = st.empty()
                    placeholder.markdown("...")

                    if backend_choice == "Ollama (Local)":
                        response = call_ollama_model(model_name, personality["prompt"], ollama_history_str, agent_name)
                    else: # Hugging Face
                        pipe = pipelines[model_name]
                        response = call_hf_model(pipe, personality["prompt"], hf_history, agent_name)

                    if response:
                        placeholder.markdown(response)
                        # Mise √† jour des historiques
                        ollama_history_str += f"\n- **{agent_name}**: {response}"
                        hf_history.append({"role": "assistant", "content": response})
                        conversation_log.append({"agent": agent_name, "modele": model_name, "reponse": response})
                    else:
                        st.error(f"L'agent {agent_name} n'a pas pu r√©pondre.")
                        break
                time.sleep(1)
            if not response: break

    st.success("D√©bat termin√© !")
    st.download_button("üì• T√©l√©charger en JSON", json.dumps(conversation_log, indent=2), "debat.json", "application/json")